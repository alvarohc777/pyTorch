{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import plotly.express as px\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional.classification import binary_stat_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "hidden_dim = 20\n",
    "n_signals = 3\n",
    "N = 64\n",
    "\n",
    "# _batch_size => m in figure 1.\n",
    "train_batch_size = 64\n",
    "dev_batch_size = 16\n",
    "test_batch_size = 16\n",
    "\n",
    "# Classification type (binary)\n",
    "tagset_size = 1\n",
    "\n",
    "# Set\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "\n",
    "class FaultDetector(nn.Module):\n",
    "    \"\"\"Information about FaultDetector\"\"\"\n",
    "\n",
    "    def __init__(self, n_signals, hidden_dim, tagset_size):\n",
    "        super(FaultDetector, self).__init__()\n",
    "        self.lstm = nn.LSTM(n_signals, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        # self.norm = nn.BatchNorm1d(tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        fc_layer = self.fc(lstm_out[:,-1, :])\n",
    "        # norm_layer = self.norm(fc_layer)\n",
    "        \n",
    "\n",
    "        return torch.sigmoid(fc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaultDetector(n_signals, hidden_dim, tagset_size).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Learning rate decay (optional)\n",
    "decayRate = 0.96\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer=optimizer, gamma=decayRate\n",
    ")\n",
    "\n",
    "print(f\"Model structure: {model}\\n\")\n",
    "\n",
    "# Number of parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Number of parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.signalload import CSV_pandas_path\n",
    "# from utils.auxfunctions import moving_window\n",
    "from utils_tesis.signalload import CSV_pandas_path\n",
    "from utils_tesis.auxfunctions import moving_window\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "import random\n",
    "\n",
    "\n",
    "class Form1Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about Form1Dataset\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir,\n",
    "        signal_names,\n",
    "        max_window_idx=65,\n",
    "        window_length=64,\n",
    "        step=1,\n",
    "        test=False,\n",
    "        dataset_size=999999,\n",
    "    ):\n",
    "        super(Form1Dataset, self).__init__()\n",
    "\n",
    "        self.signal_names = signal_names\n",
    "        self.max_window_idx = max_window_idx\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.test = test\n",
    "\n",
    "        # Find all csv in folders of DataSet\n",
    "        file_set = set()\n",
    "        for dir_, _, files in os.walk(dataset_dir):\n",
    "            for file_name in files:\n",
    "                rel_dir = os.path.relpath(dir_, dataset_dir)\n",
    "                rel_file = os.path.join(rel_dir, file_name)\n",
    "                file_set.add(rel_file)\n",
    "        csv_list = list(file_set)\n",
    "\n",
    "        if dataset_size < len(csv_list):\n",
    "            csv_list = random.sample(csv_list, dataset_size)\n",
    "            # csv_list = csv_list[:dataset_size]\n",
    "        self.csv_list = csv_list\n",
    "        self.csv_amount = len(csv_list)\n",
    "        self.windows_amount = max_window_idx * self.csv_amount\n",
    "        self.window_length = window_length\n",
    "        self.step = step\n",
    "\n",
    "    def __getitem__(self, index, data_plot=False):\n",
    "\n",
    "        # sample_settings\n",
    "        window_length = self.window_length\n",
    "        step = self.step\n",
    "\n",
    "        # calculate window_idx and filename\n",
    "        window_idx = index % self.max_window_idx\n",
    "        csv_idx = index // self.max_window_idx\n",
    "        csv_full_path = f\"{self.dataset_dir}\\{self.csv_list[csv_idx]}\"\n",
    "\n",
    "        # Load CSV, signal and create windows\n",
    "        csv_name = os.path.basename(csv_full_path)\n",
    "        signals_windows = np.empty((self.window_length, 0))\n",
    "\n",
    "        # Load Change Event\n",
    "\n",
    "        if \"L_T\" in self.csv_list[csv_idx]:\n",
    "            for signal_name in self.signal_names:\n",
    "                signal, t, _ = CSV_pandas_path(csv_full_path).load_data(signal_name)\n",
    "                # select window\n",
    "                len_ = len(signal)\n",
    "                signal_window = signal[\n",
    "                    -self.window_length\n",
    "                    - self.max_window_idx\n",
    "                    + window_idx\n",
    "                    - 1 : -self.max_window_idx\n",
    "                    + window_idx\n",
    "                    - 1\n",
    "                ]\n",
    "                signal_window = np.expand_dims(signal_window, axis=1)\n",
    "                signals_windows = np.append(signals_windows, signal_window, axis=1)\n",
    "\n",
    "            t_window = moving_window(t, window_length, step)\n",
    "            t_window = t_window[window_idx]\n",
    "            # Create labels\n",
    "\n",
    "            label = np.array([0])\n",
    "\n",
    "        # Fault Event\n",
    "        elif \"F_T\" in self.csv_list[csv_idx]:\n",
    "\n",
    "            # For faults indices 0 and 1 don't contain fault events.\n",
    "            for signal_name in self.signal_names:\n",
    "                signal, t, _ = CSV_pandas_path(csv_full_path).load_data(signal_name)\n",
    "                # select window\n",
    "                signal_window = signal[\n",
    "                    -self.window_length\n",
    "                    - self.max_window_idx\n",
    "                    + window_idx\n",
    "                     : -self.max_window_idx\n",
    "                    + window_idx\n",
    "                    \n",
    "                ]\n",
    "                signal_window = np.expand_dims(signal_window, axis=1)\n",
    "                signals_windows = np.append(signals_windows, signal_window, axis=1)\n",
    "\n",
    "            t_window = moving_window(t, window_length, step)\n",
    "            t_window = t_window[window_idx]\n",
    "            # Create labels\n",
    "            label = np.array([1])\n",
    "\n",
    "        signals_windows = torch.from_numpy(np.copy(signals_windows)).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "        # For Dataset Visualization\n",
    "        if data_plot == True:\n",
    "            return signals_windows.reshape(-1).numpy(), t_window\n",
    "        if self.test == True:\n",
    "            return signals_windows, t_window, label, csv_name, index, window_idx\n",
    "\n",
    "        return signals_windows, label\n",
    "\n",
    "    def get_event(self, csv_idx):\n",
    "\n",
    "        # Get indices\n",
    "        idx_min = csv_idx * self.max_window_idx\n",
    "        idx_max = ((csv_idx + 1) * self.max_window_idx) - 1\n",
    "\n",
    "        # Get path of csv_index\n",
    "        csv_full_path = f\"{self.dataset_dir}\\{self.csv_list[csv_idx]}\"\n",
    "\n",
    "        # Load CSV, signal and create windows\n",
    "        csv_name = os.path.basename(csv_full_path)\n",
    "        signals = np.zeros((0, 0))\n",
    "        for idx, signal_name in enumerate(self.signal_names):\n",
    "            signal, t, _ = CSV_pandas_path(csv_full_path).load_data(signal_name)\n",
    "            signal = np.expand_dims(signal, axis=1)\n",
    "            if idx == 0:\n",
    "                signals = np.empty((len(signal), 0))\n",
    "            signals = np.append(signals, signal, axis=1)\n",
    "        signal = signals\n",
    "        return signal, t, idx_min, idx_max, csv_name\n",
    "\n",
    "    def len_events(self):\n",
    "        return self.csv_amount\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv_amount * self.max_window_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_labels(pred_label, true_label):\n",
    "    label = \"\"\n",
    "    if int(pred_label) == int(true_label):\n",
    "        label += \"T\"\n",
    "    else:\n",
    "        label += \"F\"\n",
    "    if pred_label == 1:\n",
    "        label += \"P\"\n",
    "    else:\n",
    "        label += \"N\"\n",
    "    return label\n",
    "\n",
    "\n",
    "confusion_matrix_pandas = np.vectorize(confusion_matrix_labels)\n",
    "\n",
    "\n",
    "def confusion_matrix(\n",
    "    preds: torch.FloatTensor, labels: torch.FloatTensor\n",
    ") -> pd.DataFrame:\n",
    "    preds = preds.detach()\n",
    "    labels = labels.detach()\n",
    "    data = {\n",
    "        \"Pred probability\": torch.reshape(preds, (-1,)).cpu().numpy(),\n",
    "        \"Pred label\": torch.reshape(torch.round(preds), (-1,)).int().cpu().numpy(),\n",
    "        \"True label\": torch.reshape(labels, (-1,)).int().cpu().numpy(),\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"Result\"] = confusion_matrix_pandas(df[\"Pred label\"], df[\"True label\"])\n",
    "    return df\n",
    "\n",
    "def conf_matrix_metrics(conf_matrix: torch.LongTensor) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dictionary with metrics from a confusion matrix.\n",
    "\n",
    "            Parameters:\n",
    "                    conf_matrix (torch.Tensor): confusion matrix of dimension (1, 5)\n",
    "                        [TP, FP, TN, FN, TP + FN]\n",
    "\n",
    "            Returns:\n",
    "                    metrics (dict): dictionary with following metrics:\n",
    "                        metrics[\"TOTAL\"] -> total amount of samples.\n",
    "                        metrics[\"TPR\"]   -> True Positive Rate,  sensibility, recall, hit-rate.\n",
    "                        metrics[\"FPR\"]   -> False Positive Rate, Fallout.\n",
    "                        metrics[\"TNR\"]   -> True Negative Rate,  specificity, selectivity\n",
    "                        metrics[\"ACC\"]   -> Accuracy.\n",
    "                        metrics[\"PPV\"]   -> Positive Predictive Value, Precision.\n",
    "    \"\"\"\n",
    "    if conf_matrix.shape == (5,):\n",
    "        conf_matrix = np.expand_dims(conf_matrix, axis=0)\n",
    "\n",
    "    metrics = {}\n",
    "    TP = int(conf_matrix[0, 0].item())\n",
    "    FP = int(conf_matrix[0, 1].item())\n",
    "    TN = int(conf_matrix[0, 2].item())\n",
    "    FN = int(conf_matrix[0, 3].item())\n",
    "    metrics[\"TP\"] = TP\n",
    "    metrics[\"FP\"] = FP\n",
    "    metrics[\"TN\"] = TN\n",
    "    metrics[\"FN\"] = FN\n",
    "    P = TP + FN\n",
    "    N = TN + FP\n",
    "    TOTAL = TP + FP + TN + FN\n",
    "    metrics[\"TOTAL\"] = TOTAL\n",
    "    try:\n",
    "        metrics[\"TPR\"] = TP / (TP + FN)\n",
    "    except ZeroDivisionError:\n",
    "        metrics[\"TPR\"] = \"ZeroDivisionError\"\n",
    "    try:\n",
    "        metrics[\"FPR\"] = FP / (FP + TN)\n",
    "        metrics[\"TNR\"] = TN / (FP + TN)\n",
    "    except ZeroDivisionError:\n",
    "        metrics[\"FPR\"] = \"ZeroDivisionError\"\n",
    "        metrics[\"TNR\"] = \"ZeroDivisionError\"\n",
    "    metrics[\"ACC\"] = (TP + TN) / (TOTAL)\n",
    "    try:\n",
    "        metrics[\"PPV\"] = TP / (TP + FP)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"No se puede obtener PPV, división por cero\")\n",
    "    return metrics\n",
    "\n",
    "def signal_exploration(idx: int, dataset, model, plot_signal: bool = True):\n",
    "    signal, t, idx_min, idx_max, csv_name = dataset.get_event(idx)\n",
    "    model.eval()\n",
    "    if plot_signal == True:\n",
    "        plt.plot(t, signal)\n",
    "        plt.show()\n",
    "    conf_matrix = torch.zeros(1, 5, dtype=torch.int64).to(device)\n",
    "    preds = torch.empty((0, 1)).to(device)\n",
    "    labels = torch.empty((0, 1)).to(device)\n",
    "    for i in range(idx_min, idx_max + 1):\n",
    "        signal, y = dataset.__getitem__(i)\n",
    "        y = torch.unsqueeze(y, 0).to(device)\n",
    "        signal = torch.unsqueeze(signal, 0).to(device)\n",
    "        pred = model(signal)\n",
    "        preds = torch.cat((preds, pred), 0)\n",
    "        labels = torch.cat((labels, y), 0)\n",
    "        conf_matrix = conf_matrix.add(binary_stat_scores(pred, y))\n",
    "    df = confusion_matrix(preds, labels)\n",
    "    df.insert(loc=0, column=\"event_idx\", value=np.repeat(idx, idx_max - idx_min + 1))\n",
    "    # df.insert(loc=0, column=\"window_idx\", value=df.index)\n",
    "    # df.insert(loc=0, column=\"indices\", value=idxs)\n",
    "\n",
    "    return df, conf_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(metrics):\n",
    "    z = [[metrics[\"TP\"], metrics[\"FN\"]], [metrics[\"FP\"], metrics[\"TN\"]]]\n",
    "    fig = px.imshow(\n",
    "        z,\n",
    "        text_auto=True,\n",
    "        template=\"seaborn\",\n",
    "        labels=dict(x=\"Predicted Label\", y=\"Real Label\", color=\"Predictions\"),\n",
    "        x=[\"Positive\", \"Negative\"],\n",
    "        y=[\"Positive\", \"Negative\"],\n",
    "        width=400,\n",
    "        height=300,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(f\"{'Total windows:':.<30}{metrics['TOTAL']:4}\")\n",
    "    print(f\"{'True Positives:':.<30}{metrics['TP']:4}\")\n",
    "    print(f\"{'False Positives:':.<30}{metrics['FP']:4}\")\n",
    "    print(f\"{'True Negatives:':.<30}{metrics['TN']:4}\")\n",
    "    print(f\"{'False Negatives:':.<30}{metrics['FN']:4}\")\n",
    "    print(f\"{'Accuracy:':.<30}{metrics['ACC']*100:>6.1f}%\")\n",
    "    print(f\"{'True Positive Rate:':.<30}{metrics['TPR']*100:>6.1f}%\") \n",
    "    print(f\"{'False Positive Rate:':.<30}{metrics['FPR']*100:>6.1f}%\") \n",
    "    print(f\"{'True Negative Rate:':.<30}{metrics['TNR']*100:>6.1f}%\") \n",
    "    try:\n",
    "        print(f\"{'Positive Predictive Value:':.<30}{metrics['PPV']*100:>6.1f}%\")\n",
    "    except KeyError:\n",
    "        print(f\"PPV divided by 0. No positive class predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/R2Currents_DB2_V2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_dir = \"C:/Users/aherrada/OneDrive - Universidad del Norte/Uninorte/DetectionDataBase/LSTM_SEM\"\n",
    "current_R1A = \"I: X0023A-R1A\"\n",
    "current_R1B = \"I: X0023B-R1B\"\n",
    "current_R1C = \"I: X0023C-R1C\"\n",
    "current_R2A = \"I: X0004A-R2A\"\n",
    "current_R2B = \"I: X0004B-R2B\"\n",
    "current_R2C = \"I: X0004C-R2C\"\n",
    "current_R3A = \"I: X0071A-R3A\"\n",
    "current_R3B = \"I: X0071B-R3B\"\n",
    "current_R3C = \"I: X0071C-R3C\"\n",
    "\n",
    "voltage_R1A = \"V: R1A\"\n",
    "voltage_R1B = \"V: R1B\"\n",
    "voltage_R1C = \"V: R1C\"\n",
    "signal_names = [current_R2A, current_R2B, current_R2C]\n",
    "max_window_idx = 62\n",
    "# model.load_state_dict(torch.load('models\\R3Currents_DB2.pth'))\n",
    "\n",
    "# Create Dataset\n",
    "dataset = Form1Dataset(\n",
    "    dataset_dir, max_window_idx=max_window_idx, signal_names=signal_names, dataset_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = torch.zeros(0, 5, dtype=torch.int64).to(device)\n",
    "for idx in tqdm(range(dataset.len_events())):\n",
    "# for idx in tqdm(range()):\n",
    "    df, CM = signal_exploration(idx, dataset, model, plot_signal=False)\n",
    "    conf_matrix = torch.cat((conf_matrix, CM))\n",
    "    if idx == 0:\n",
    "        dataset_df = df\n",
    "    else:\n",
    "        dataset_df = pd.concat([dataset_df, df])\n",
    "\n",
    "dataset_df = dataset_df.reset_index()\n",
    "dataset_df = dataset_df.rename(columns={'index': 'window idx'})\n",
    "conf_matrix_total = np.sum(conf_matrix.cpu().numpy(), axis=0)\n",
    "\n",
    "\n",
    "# dataset_df = pd.read_parquet(f\"parquet_data\\R2_currents_DB2_df.parquet\")\n",
    "# conf_matrix_df = pd.read_parquet(f\"parquet_data\\R2_currents_DB2_CM_df.parquet\")\n",
    "# conf_matrix = np.load(f\"parquet_data\\R2_currents_DB2_CM.npy\", allow_pickle=False)\n",
    "# conf_matrix_total = np.sum(conf_matrix, axis=0)\n",
    "# metrics = conf_matrix_metrics(conf_matrix_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df and plots settings\n",
    "pd.set_option(\"display.float_format\", \"{:.2%}\".format)\n",
    "\n",
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "\n",
    "\n",
    "# print(dataset_df)\n",
    "pd.set_option(\"display.max_rows\", 2000)\n",
    "false_positive = dataset_df.query('Result == \"FP\"')\n",
    "false_negative = dataset_df.query('Result == \"FN\"')\n",
    "# sample_df = pd.concat([sample_df, sample_df])\n",
    "\n",
    "false_positive_plot = false_positive.groupby([\"window idx\"])[\"window idx\"].count()\n",
    "false_negative_plot = false_negative.groupby([\"window idx\"])[\"window idx\"].count()\n",
    "# print(false_positive['Pred probability'].value_counts(bins=10, sort=False))\n",
    "\n",
    "\n",
    "if len(false_positive_plot) > 0:\n",
    "    print(\"FALSE POSITVES\")\n",
    "    print(false_positive)\n",
    "    # print(false_positive.groupby(['event_idx'])['event_idx'].count())\n",
    "    false_positive.groupby(['event_idx'])['event_idx'].count().plot(kind='bar')\n",
    "    plt.show()\n",
    "    false_positive_plot.plot(kind=\"bar\", edgecolor=\"black\")\n",
    "    plt.show()\n",
    "    false_positive[\"Pred probability\"].value_counts(\n",
    "        bins = [i * 0.1 for i in range(5, 11)], sort=False, normalize=True\n",
    "    ).plot(kind=\"bar\")\n",
    "    plt.show()\n",
    "if len(false_negative_plot) > 0:\n",
    "    print(\"FALSE NEGATIVES\")\n",
    "    print(false_negative)\n",
    "    # print(false_negative.groupby(['event_idx'])['event_idx'].count())\n",
    "    false_negative.groupby(['event_idx'])['event_idx'].count().plot(kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "    false_negative_plot.plot(kind=\"bar\", edgecolor=\"black\")\n",
    "    plt.show()\n",
    "    false_negative[\"Pred probability\"].value_counts(\n",
    "        bins = [i * 0.1 for i in range(6)], sort=False, normalize=True\n",
    "    ).plot(kind=\"bar\")\n",
    "    plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models\\LSTMHarmonic_weights_R3Currents.pth'))\n",
    "print(conf_matrix_total)\n",
    "metrics = conf_matrix_metrics(conf_matrix_total)\n",
    "print_metrics(metrics)\n",
    "plot_confusion_matrix(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_idx = 3\n",
    "print(dataset.csv_list[event_idx])\n",
    "sample_df, conf_matrix = signal_exploration(event_idx, dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_idx = 47\n",
    "sample_idx = (max_window_idx * event_idx) + window_idx\n",
    "\n",
    "\n",
    "def update_fig(fig):\n",
    "    fig.update_traces(line_color=\"#EEEEEE\", line_width=2)\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=\"#222831\",\n",
    "        plot_bgcolor=\"#393E46\",\n",
    "        font_color=\"whitesmoke\",\n",
    "    )\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor=\"#32E0C4\")\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor=\"#32E0C4\")\n",
    "\n",
    "\n",
    "signal, t = dataset.__getitem__(sample_idx, data_plot=True)\n",
    "signal = np.copy(signal)\n",
    "signal = signal.reshape((-1, 3))\n",
    "signal = signal[:, 0]\n",
    "t = np.copy(t)\n",
    "print(type(t))\n",
    "print(type(signal))\n",
    "# print(f\"Window index: {sample_df.query('indices == 196').index[0]}\")\n",
    "fig = px.line(\n",
    "    x=t,\n",
    "    y=signal,\n",
    "    width=600,\n",
    "    height=400,\n",
    "    labels=dict(x=\"time\", y=\"Amplitude\"),\n",
    ")\n",
    "# update_fig(fig)x\n",
    "fig.show()\n",
    "fig = px.line(\n",
    "    x=sample_df.index,\n",
    "    y=sample_df[\"Pred label\"],\n",
    "    width=600,\n",
    "    height=400,\n",
    "    labels=dict(x=\"Window\", y=\"Trip Signal\"),\n",
    "    line_shape=\"hv\",\n",
    ")\n",
    "# update_fig(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_parquet('parquet_data/R2_currents_DB2_V2df.parquet')\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix.cpu(), columns = ['TP','FP','TF', 'FN', 'TP + FN'])\n",
    "conf_matrix_df.to_parquet('parquet_data/R2_currents_DB2_CM_df_V2.parquet')\n",
    "np.save('parquet_data/R2_currents_DB2_CM_V2.npy', conf_matrix.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
